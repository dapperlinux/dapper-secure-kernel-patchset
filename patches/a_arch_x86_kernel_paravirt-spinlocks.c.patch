diff --git a/arch/x86/kernel/paravirt-spinlocks.c b/arch/x86/kernel/paravirt-spinlocks.c
index 8f2d1c9..f4b150b 100644
--- a/arch/x86/kernel/paravirt-spinlocks.c
+++ b/arch/x86/kernel/paravirt-spinlocks.c
@@ -16,7 +16,7 @@ PV_CALLEE_SAVE_REGS_THUNK(__native_queued_spin_unlock);
 
 bool pv_is_native_spin_unlock(void)
 {
-	return pv_lock_ops.queued_spin_unlock.func ==
+	return pv_lock_ops.queued_spin_unlock.queued_spin_unlock ==
 		__raw_callee_save___native_queued_spin_unlock;
 }
 
@@ -32,12 +32,22 @@ bool pv_is_native_vcpu_is_preempted(void)
 		__raw_callee_save___native_vcpu_is_preempted;
 }
 
-struct pv_lock_ops pv_lock_ops = {
+#ifdef CONFIG_SMP
+static void native_wait(u8 *ptr, u8 val)
+{
+}
+
+static void native_kick(int cpu)
+{
+}
+#endif /* SMP */
+
+struct pv_lock_ops pv_lock_ops __read_only = {
 #ifdef CONFIG_SMP
 	.queued_spin_lock_slowpath = native_queued_spin_lock_slowpath,
-	.queued_spin_unlock = PV_CALLEE_SAVE(__native_queued_spin_unlock),
-	.wait = paravirt_nop,
-	.kick = paravirt_nop,
+	.queued_spin_unlock = PV_CALLEE_SAVE(queued_spin_unlock, __native_queued_spin_unlock),
+	.wait = native_wait,
+	.kick = native_kick,
 	.vcpu_is_preempted = PV_CALLEE_SAVE(__native_vcpu_is_preempted),
 #endif /* SMP */
 };
