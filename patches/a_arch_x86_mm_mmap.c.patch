diff --git a/arch/x86/mm/mmap.c b/arch/x86/mm/mmap.c
index d2dc043..41dfc2b 100644
--- a/arch/x86/mm/mmap.c
+++ b/arch/x86/mm/mmap.c
@@ -88,11 +88,17 @@ unsigned long arch_mmap_rnd(void)
 	return arch_rnd(mmap_is_ia32() ? mmap32_rnd_bits : mmap64_rnd_bits);
 }
 
-static unsigned long mmap_base(unsigned long rnd, unsigned long task_size)
+static unsigned long mmap_base(struct mm_struct *mm, unsigned long rnd, unsigned long task_size)
 {
 	unsigned long gap = rlimit(RLIMIT_STACK);
 	unsigned long pad = stack_maxrandom_size(task_size) + stack_guard_gap;
 	unsigned long gap_min, gap_max;
+	unsigned long pax_task_size = TASK_SIZE;
+
+#ifdef CONFIG_PAX_SEGMEXEC
+	if (mm->pax_flags & MF_PAX_SEGMEXEC)
+		pax_task_size = SEGMEXEC_TASK_SIZE;
+#endif
 
 	/* Values close to RLIM_INFINITY can overflow. */
 	if (gap + pad > gap)
@@ -103,19 +109,23 @@ static unsigned long mmap_base(unsigned long rnd, unsigned long task_size)
 	 * Leave an at least ~128 MB hole with possible stack randomization.
 	 */
 	gap_min = SIZE_128M;
-	gap_max = (task_size / 6) * 5;
+	gap_max = (pax_task_size / 6) * 5;
 
 	if (gap < gap_min)
 		gap = gap_min;
 	else if (gap > gap_max)
 		gap = gap_max;
 
-	return PAGE_ALIGN(task_size - gap - rnd);
+	return PAGE_ALIGN(pax_task_size - gap - rnd);
 }
 
-static unsigned long mmap_legacy_base(unsigned long rnd,
+static unsigned long mmap_legacy_base(struct mm_struct *mm, unsigned long rnd,
 				      unsigned long task_size)
 {
+#ifdef CONFIG_PAX_SEGMEXEC
+	if (mmap_is_ia32() && (mm->pax_flags & MF_PAX_SEGMEXEC))
+		return SEGMEXEC_TASK_UNMAPPED_BASE + rnd;
+#endif
 	return __TASK_UNMAPPED_BASE(task_size) + rnd;
 }
 
@@ -123,14 +133,14 @@ static unsigned long mmap_legacy_base(unsigned long rnd,
  * This function, called very early during the creation of a new
  * process VM image, sets up which VM layout function to use:
  */
-static void arch_pick_mmap_base(unsigned long *base, unsigned long *legacy_base,
+static void arch_pick_mmap_base(struct mm_struct *mm, unsigned long *base, unsigned long *legacy_base,
 		unsigned long random_factor, unsigned long task_size)
 {
-	*legacy_base = mmap_legacy_base(random_factor, task_size);
+	*legacy_base = mmap_legacy_base(mm, random_factor, task_size);
 	if (mmap_is_legacy())
 		*base = *legacy_base;
 	else
-		*base = mmap_base(random_factor, task_size);
+		*base = mmap_base(mm, random_factor, task_size);
 }
 
 void arch_pick_mmap_layout(struct mm_struct *mm)
@@ -140,7 +150,7 @@ void arch_pick_mmap_layout(struct mm_struct *mm)
 	else
 		mm->get_unmapped_area = arch_get_unmapped_area_topdown;
 
-	arch_pick_mmap_base(&mm->mmap_base, &mm->mmap_legacy_base,
+	arch_pick_mmap_base(mm, &mm->mmap_base, &mm->mmap_legacy_base,
 			arch_rnd(mmap64_rnd_bits), tasksize_64bit());
 
 #ifdef CONFIG_HAVE_ARCH_COMPAT_MMAP_BASES
@@ -153,6 +163,14 @@ void arch_pick_mmap_layout(struct mm_struct *mm)
 	arch_pick_mmap_base(&mm->mmap_compat_base, &mm->mmap_compat_legacy_base,
 			arch_rnd(mmap32_rnd_bits), tasksize_32bit());
 #endif
+
+#ifdef CONFIG_PAX_RANDMMAP
+	if (mm->pax_flags & MF_PAX_RANDMMAP) {
+		mm->mmap_legacy_base += mm->delta_mmap;
+		mm->mmap_base -= mm->delta_mmap + mm->delta_stack;
+	}
+#endif
+
 }
 
 unsigned long get_mmap_base(int is_legacy)
