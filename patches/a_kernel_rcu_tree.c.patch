diff --git a/kernel/rcu/tree.c b/kernel/rcu/tree.c
index 50fee76..f71f17e 100644
--- a/kernel/rcu/tree.c
+++ b/kernel/rcu/tree.c
@@ -297,7 +297,7 @@ static void rcu_dynticks_eqs_enter(void)
 	 * critical sections, and we also must force ordering with the
 	 * next idle sojourn.
 	 */
-	special = atomic_inc_return(&rdtp->dynticks);
+	special = atomic_inc_return_unchecked(&rdtp->dynticks);
 	WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && special & 0x1);
 }
 
@@ -315,7 +315,7 @@ static void rcu_dynticks_eqs_exit(void)
 	 * and we also must force ordering with the next RCU read-side
 	 * critical section.
 	 */
-	special = atomic_inc_return(&rdtp->dynticks);
+	special = atomic_inc_return_unchecked(&rdtp->dynticks);
 	WARN_ON_ONCE(IS_ENABLED(CONFIG_RCU_EQS_DEBUG) && !(special & 0x1));
 }
 
@@ -333,9 +333,9 @@ static void rcu_dynticks_eqs_online(void)
 {
 	struct rcu_dynticks *rdtp = this_cpu_ptr(&rcu_dynticks);
 
-	if (atomic_read(&rdtp->dynticks) & 0x1)
+	if (atomic_read_unchecked(&rdtp->dynticks) & 0x1)
 		return;
-	atomic_add(0x1, &rdtp->dynticks);
+	atomic_add_unchecked(0x1, &rdtp->dynticks);
 }
 
 /*
@@ -347,7 +347,7 @@ bool rcu_dynticks_curr_cpu_in_eqs(void)
 {
 	struct rcu_dynticks *rdtp = this_cpu_ptr(&rcu_dynticks);
 
-	return !(atomic_read(&rdtp->dynticks) & 0x1);
+	return !(atomic_read_unchecked(&rdtp->dynticks) & 0x1);
 }
 
 /*
@@ -356,7 +356,7 @@ bool rcu_dynticks_curr_cpu_in_eqs(void)
  */
 int rcu_dynticks_snap(struct rcu_dynticks *rdtp)
 {
-	int snap = atomic_add_return(0, &rdtp->dynticks);
+	int snap = atomic_add_return_unchecked(0, &rdtp->dynticks);
 
 	return snap;
 }
@@ -387,7 +387,7 @@ static bool rcu_dynticks_in_eqs_since(struct rcu_dynticks *rdtp, int snap)
 static void rcu_dynticks_momentary_idle(void)
 {
 	struct rcu_dynticks *rdtp = this_cpu_ptr(&rcu_dynticks);
-	int special = atomic_add_return(2, &rdtp->dynticks);
+	int special = atomic_add_return_unchecked(2, &rdtp->dynticks);
 
 	/* It is illegal to call this from idle state. */
 	WARN_ON_ONCE(!(special & 0x1));
@@ -3115,7 +3115,7 @@ __rcu_process_callbacks(struct rcu_state *rsp)
 /*
  * Do RCU core processing for the current CPU.
  */
-static __latent_entropy void rcu_process_callbacks(struct softirq_action *unused)
+static __latent_entropy void rcu_process_callbacks(void)
 {
 	struct rcu_state *rsp;
 
@@ -3670,7 +3670,7 @@ static void _rcu_barrier_trace(struct rcu_state *rsp, const char *s,
 			       int cpu, unsigned long done)
 {
 	trace_rcu_barrier(rsp->name, s, cpu,
-			  atomic_read(&rsp->barrier_cpu_count), done);
+			  atomic_read_unchecked(&rsp->barrier_cpu_count), done);
 }
 
 /*
@@ -3682,7 +3682,7 @@ static void rcu_barrier_callback(struct rcu_head *rhp)
 	struct rcu_data *rdp = container_of(rhp, struct rcu_data, barrier_head);
 	struct rcu_state *rsp = rdp->rsp;
 
-	if (atomic_dec_and_test(&rsp->barrier_cpu_count)) {
+	if (atomic_dec_and_test_unchecked(&rsp->barrier_cpu_count)) {
 		_rcu_barrier_trace(rsp, "LastCB", -1, rsp->barrier_sequence);
 		complete(&rsp->barrier_completion);
 	} else {
@@ -3699,7 +3699,7 @@ static void rcu_barrier_func(void *type)
 	struct rcu_data *rdp = raw_cpu_ptr(rsp->rda);
 
 	_rcu_barrier_trace(rsp, "IRQ", -1, rsp->barrier_sequence);
-	atomic_inc(&rsp->barrier_cpu_count);
+	atomic_inc_unchecked(&rsp->barrier_cpu_count);
 	rsp->call(&rdp->barrier_head, rcu_barrier_callback);
 }
 
@@ -3737,7 +3737,7 @@ static void _rcu_barrier(struct rcu_state *rsp)
 	 * to ensure that no offline CPU has callbacks queued.
 	 */
 	init_completion(&rsp->barrier_completion);
-	atomic_set(&rsp->barrier_cpu_count, 1);
+	atomic_set_unchecked(&rsp->barrier_cpu_count, 1);
 	get_online_cpus();
 
 	/*
@@ -3757,7 +3757,7 @@ static void _rcu_barrier(struct rcu_state *rsp)
 				_rcu_barrier_trace(rsp, "OnlineNoCB", cpu,
 						   rsp->barrier_sequence);
 				smp_mb__before_atomic();
-				atomic_inc(&rsp->barrier_cpu_count);
+				atomic_inc_unchecked(&rsp->barrier_cpu_count);
 				__call_rcu(&rdp->barrier_head,
 					   rcu_barrier_callback, rsp, cpu, 0);
 			}
@@ -3776,7 +3776,7 @@ static void _rcu_barrier(struct rcu_state *rsp)
 	 * Now that we have an rcu_barrier_callback() callback on each
 	 * CPU, and thus each counted, remove the initial count.
 	 */
-	if (atomic_dec_and_test(&rsp->barrier_cpu_count))
+	if (atomic_dec_and_test_unchecked(&rsp->barrier_cpu_count))
 		complete(&rsp->barrier_completion);
 
 	/* Wait for all rcu_barrier_callback() callbacks to be invoked. */
