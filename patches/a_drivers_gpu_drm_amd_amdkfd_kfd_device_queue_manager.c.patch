diff --git a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.c b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.c
index f49c551..ad74c7e 100644
--- a/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.c
+++ b/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.c
@@ -254,7 +254,7 @@ static int create_compute_queue_nocpsch(struct device_queue_manager *dqm,
 	int retval;
 	struct mqd_manager *mqd;
 
-	mqd = dqm->ops.get_mqd_manager(dqm, KFD_MQD_TYPE_COMPUTE);
+	mqd = dqm->ops->get_mqd_manager(dqm, KFD_MQD_TYPE_COMPUTE);
 	if (!mqd)
 		return -ENOMEM;
 
@@ -288,14 +288,14 @@ static int destroy_queue_nocpsch(struct device_queue_manager *dqm,
 	mutex_lock(&dqm->lock);
 
 	if (q->properties.type == KFD_QUEUE_TYPE_COMPUTE) {
-		mqd = dqm->ops.get_mqd_manager(dqm, KFD_MQD_TYPE_COMPUTE);
+		mqd = dqm->ops->get_mqd_manager(dqm, KFD_MQD_TYPE_COMPUTE);
 		if (mqd == NULL) {
 			retval = -ENOMEM;
 			goto out;
 		}
 		deallocate_hqd(dqm, q);
 	} else if (q->properties.type == KFD_QUEUE_TYPE_SDMA) {
-		mqd = dqm->ops.get_mqd_manager(dqm, KFD_MQD_TYPE_SDMA);
+		mqd = dqm->ops->get_mqd_manager(dqm, KFD_MQD_TYPE_SDMA);
 		if (mqd == NULL) {
 			retval = -ENOMEM;
 			goto out;
@@ -357,7 +357,7 @@ static int update_queue(struct device_queue_manager *dqm, struct queue *q)
 	bool prev_active = false;
 
 	mutex_lock(&dqm->lock);
-	mqd = dqm->ops.get_mqd_manager(dqm,
+	mqd = dqm->ops->get_mqd_manager(dqm,
 			get_mqd_type_from_queue_type(q->properties.type));
 	if (!mqd) {
 		retval = -ENOMEM;
@@ -414,7 +414,7 @@ static int register_process_nocpsch(struct device_queue_manager *dqm,
 	mutex_lock(&dqm->lock);
 	list_add(&n->list, &dqm->queues);
 
-	retval = dqm->ops_asic_specific.register_process(dqm, qpd);
+	retval = dqm->ops_asic_specific->register_process(dqm, qpd);
 
 	dqm->processes_count++;
 
@@ -635,7 +635,7 @@ static int create_sdma_queue_nocpsch(struct device_queue_manager *dqm,
 	struct mqd_manager *mqd;
 	int retval;
 
-	mqd = dqm->ops.get_mqd_manager(dqm, KFD_MQD_TYPE_SDMA);
+	mqd = dqm->ops->get_mqd_manager(dqm, KFD_MQD_TYPE_SDMA);
 	if (!mqd)
 		return -ENOMEM;
 
@@ -584,7 +584,7 @@ static int create_sdma_queue_nocpsch(struct device_queue_manager *dqm,
 	pr_debug("SDMA queue id: %d\n", q->properties.sdma_queue_id);
 	pr_debug("SDMA engine id: %d\n", q->properties.sdma_engine_id);
 
-	dqm->ops_asic_specific.init_sdma_vm(dqm, q, qpd);
+	dqm->ops_asic_specific->init_sdma_vm(dqm, q, qpd);
 	retval = mqd->init_mqd(mqd, &q->mqd, &q->mqd_mem_obj,
 				&q->gart_mqd_addr, &q->properties);
 	if (retval)
@@ -661,7 +661,7 @@ static int initialize_cpsch(struct device_queue_manager *dqm)
 	dqm->queue_count = dqm->processes_count = 0;
 	dqm->sdma_queue_count = 0;
 	dqm->active_runlist = false;
-	retval = dqm->ops_asic_specific.initialize(dqm);
+	retval = dqm->ops_asic_specific->initialize(dqm);
 	if (retval)
 		mutex_destroy(&dqm->lock);
 
@@ -810,7 +810,7 @@ static int create_queue_cpsch(struct device_queue_manager *dqm, struct queue *q,
 	if (q->properties.type == KFD_QUEUE_TYPE_SDMA)
 		select_sdma_engine_id(q);
 
-	mqd = dqm->ops.get_mqd_manager(dqm,
+	mqd = dqm->ops->get_mqd_manager(dqm,
 			get_mqd_type_from_queue_type(q->properties.type));
 
 	if (!mqd) {
@@ -818,7 +818,7 @@ static int create_queue_cpsch(struct device_queue_manager *dqm, struct queue *q,
 		goto out;
 	}
 
-	dqm->ops_asic_specific.init_sdma_vm(dqm, q, qpd);
+	dqm->ops_asic_specific->init_sdma_vm(dqm, q, qpd);
 	retval = mqd->init_mqd(mqd, &q->mqd, &q->mqd_mem_obj,
 				&q->gart_mqd_addr, &q->properties);
 	if (retval)
@@ -1060,7 +1060,7 @@ static int destroy_queue_cpsch(struct device_queue_manager *dqm,
 
 	}
 
-	mqd = dqm->ops.get_mqd_manager(dqm,
+	mqd = dqm->ops->get_mqd_manager(dqm,
 			get_mqd_type_from_queue_type(q->properties.type));
 	if (!mqd) {
 		retval = -ENOMEM;
@@ -1149,7 +1149,7 @@ static bool set_cache_memory_policy(struct device_queue_manager *dqm,
 		qpd->sh_mem_ape1_limit = limit >> 16;
 	}
 
-	retval = dqm->ops_asic_specific.set_cache_memory_policy(
+	retval = dqm->ops_asic_specific->set_cache_memory_policy(
 			dqm,
 			qpd,
 			default_policy,
@@ -1088,6 +1088,36 @@ static bool set_cache_memory_policy(struct device_queue_manager *dqm,
 	return retval;
 }
 
+static const struct device_queue_manager_ops cp_dqm_ops = {
+	.create_queue = create_queue_cpsch,
+	.initialize = initialize_cpsch,
+	.start = start_cpsch,
+	.stop = stop_cpsch,
+	.destroy_queue = destroy_queue_cpsch,
+	.update_queue = update_queue,
+	.get_mqd_manager = get_mqd_manager_nocpsch,
+	.register_process = register_process_nocpsch,
+	.unregister_process = unregister_process_nocpsch,
+	.uninitialize = uninitialize_nocpsch,
+	.create_kernel_queue = create_kernel_queue_cpsch,
+	.destroy_kernel_queue = destroy_kernel_queue_cpsch,
+	.set_cache_memory_policy = set_cache_memory_policy,
+};
+
+static const struct device_queue_manager_ops no_cp_dqm_ops = {
+	.start = start_nocpsch,
+	.stop = stop_nocpsch,
+	.create_queue = create_queue_nocpsch,
+	.destroy_queue = destroy_queue_nocpsch,
+	.update_queue = update_queue,
+	.get_mqd_manager = get_mqd_manager_nocpsch,
+	.register_process = register_process_nocpsch,
+	.unregister_process = unregister_process_nocpsch,
+	.initialize = initialize_nocpsch,
+	.uninitialize = uninitialize_nocpsch,
+	.set_cache_memory_policy = set_cache_memory_policy,
+};
+
 struct device_queue_manager *device_queue_manager_init(struct kfd_dev *dev)
 {
 	struct device_queue_manager *dqm;
@@ -1103,33 +1133,11 @@ struct device_queue_manager *device_queue_manager_init(struct kfd_dev *dev)
 	case KFD_SCHED_POLICY_HWS:
 	case KFD_SCHED_POLICY_HWS_NO_OVERSUBSCRIPTION:
 		/* initialize dqm for cp scheduling */
-		dqm->ops.create_queue = create_queue_cpsch;
-		dqm->ops.initialize = initialize_cpsch;
-		dqm->ops.start = start_cpsch;
-		dqm->ops.stop = stop_cpsch;
-		dqm->ops.destroy_queue = destroy_queue_cpsch;
-		dqm->ops.update_queue = update_queue;
-		dqm->ops.get_mqd_manager = get_mqd_manager_nocpsch;
-		dqm->ops.register_process = register_process_nocpsch;
-		dqm->ops.unregister_process = unregister_process_nocpsch;
-		dqm->ops.uninitialize = uninitialize_nocpsch;
-		dqm->ops.create_kernel_queue = create_kernel_queue_cpsch;
-		dqm->ops.destroy_kernel_queue = destroy_kernel_queue_cpsch;
-		dqm->ops.set_cache_memory_policy = set_cache_memory_policy;
+		dqm->ops = &cp_dqm_ops;
 		break;
 	case KFD_SCHED_POLICY_NO_HWS:
 		/* initialize dqm for no cp scheduling */
-		dqm->ops.start = start_nocpsch;
-		dqm->ops.stop = stop_nocpsch;
-		dqm->ops.create_queue = create_queue_nocpsch;
-		dqm->ops.destroy_queue = destroy_queue_nocpsch;
-		dqm->ops.update_queue = update_queue;
-		dqm->ops.get_mqd_manager = get_mqd_manager_nocpsch;
-		dqm->ops.register_process = register_process_nocpsch;
-		dqm->ops.unregister_process = unregister_process_nocpsch;
-		dqm->ops.initialize = initialize_nocpsch;
-		dqm->ops.uninitialize = uninitialize_nocpsch;
-		dqm->ops.set_cache_memory_policy = set_cache_memory_policy;
+		dqm->ops = &no_cp_dqm_ops;
 		break;
 	default:
 		pr_err("Invalid scheduling policy %d\n", sched_policy);
@@ -1138,15 +1146,15 @@ struct device_queue_manager *device_queue_manager_init(struct kfd_dev *dev)
 
 	switch (dev->device_info->asic_family) {
 	case CHIP_CARRIZO:
-		device_queue_manager_init_vi(&dqm->ops_asic_specific);
+		device_queue_manager_init_vi(dqm);
 		break;
 
 	case CHIP_KAVERI:
-		device_queue_manager_init_cik(&dqm->ops_asic_specific);
+		device_queue_manager_init_cik(dqm);
 		break;
 	}
 
-	if (!dqm->ops.initialize(dqm))
+	if (!dqm->ops->initialize(dqm))
 		return dqm;
 
 out_free:
@@ -1156,6 +1164,6 @@ struct device_queue_manager *device_queue_manager_init(struct kfd_dev *dev)
 
 void device_queue_manager_uninit(struct device_queue_manager *dqm)
 {
-	dqm->ops.uninitialize(dqm);
+	dqm->ops->uninitialize(dqm);
 	kfree(dqm);
 }
