diff --git a/arch/x86/crypto/cast5-avx-x86_64-asm_64.S b/arch/x86/crypto/cast5-avx-x86_64-asm_64.S
index b4a8806..92b2eee 100644
--- a/arch/x86/crypto/cast5-avx-x86_64-asm_64.S
+++ b/arch/x86/crypto/cast5-avx-x86_64-asm_64.S
@@ -25,6 +25,7 @@
 
 #include <linux/linkage.h>
 #include <asm/frame.h>
+#include <asm/alternative-asm.h>
 
 .file "cast5-avx-x86_64-asm_64.S"
 
@@ -292,7 +293,7 @@ __cast5_enc_blk16:
 	outunpack_blocks(RR3, RL3, RTMP, RX, RKM);
 	outunpack_blocks(RR4, RL4, RTMP, RX, RKM);
 
-	ret;
+	pax_ret __cast5_enc_blk16;
 ENDPROC(__cast5_enc_blk16)
 
 .align 16
@@ -363,14 +364,14 @@ __cast5_dec_blk16:
 	outunpack_blocks(RR3, RL3, RTMP, RX, RKM);
 	outunpack_blocks(RR4, RL4, RTMP, RX, RKM);
 
-	ret;
+	pax_ret __cast5_dec_blk16;
 
 .L__skip_dec:
 	vpsrldq $4, RKR, RKR;
 	jmp .L__dec_tail;
 ENDPROC(__cast5_dec_blk16)
 
-ENTRY(cast5_ecb_enc_16way)
+RAP_ENTRY(cast5_ecb_enc_16way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -389,7 +390,7 @@ ENTRY(cast5_ecb_enc_16way)
 	vmovdqu (6*4*4)(%rdx), RL4;
 	vmovdqu (7*4*4)(%rdx), RR4;
 
-	call __cast5_enc_blk16;
+	pax_direct_call __cast5_enc_blk16;
 
 	vmovdqu RR1, (0*4*4)(%r11);
 	vmovdqu RL1, (1*4*4)(%r11);
@@ -401,10 +402,10 @@ ENTRY(cast5_ecb_enc_16way)
 	vmovdqu RL4, (7*4*4)(%r11);
 
 	FRAME_END
-	ret;
+	pax_ret cast5_ecb_enc_16way;
 ENDPROC(cast5_ecb_enc_16way)
 
-ENTRY(cast5_ecb_dec_16way)
+RAP_ENTRY(cast5_ecb_dec_16way)
 	/* input:
 	 *	%rdi: ctx, CTX
 	 *	%rsi: dst
@@ -423,7 +424,7 @@ ENTRY(cast5_ecb_dec_16way)
 	vmovdqu (6*4*4)(%rdx), RL4;
 	vmovdqu (7*4*4)(%rdx), RR4;
 
-	call __cast5_dec_blk16;
+	pax_direct_call __cast5_dec_blk16;
 
 	vmovdqu RR1, (0*4*4)(%r11);
 	vmovdqu RL1, (1*4*4)(%r11);
@@ -435,7 +436,7 @@ ENTRY(cast5_ecb_dec_16way)
 	vmovdqu RL4, (7*4*4)(%r11);
 
 	FRAME_END
-	ret;
+	pax_ret cast5_ecb_dec_16way;
 ENDPROC(cast5_ecb_dec_16way)
 
 ENTRY(cast5_cbc_dec_16way)
@@ -446,10 +447,10 @@ ENTRY(cast5_cbc_dec_16way)
 	 */
 	FRAME_BEGIN
 
-	pushq %r12;
+	pushq %r14;
 
 	movq %rsi, %r11;
-	movq %rdx, %r12;
+	movq %rdx, %r14;
 
 	vmovdqu (0*16)(%rdx), RL1;
 	vmovdqu (1*16)(%rdx), RR1;
@@ -460,19 +461,19 @@ ENTRY(cast5_cbc_dec_16way)
 	vmovdqu (6*16)(%rdx), RL4;
 	vmovdqu (7*16)(%rdx), RR4;
 
-	call __cast5_dec_blk16;
+	pax_direct_call __cast5_dec_blk16;
 
 	/* xor with src */
-	vmovq (%r12), RX;
+	vmovq (%r14), RX;
 	vpshufd $0x4f, RX, RX;
 	vpxor RX, RR1, RR1;
-	vpxor 0*16+8(%r12), RL1, RL1;
-	vpxor 1*16+8(%r12), RR2, RR2;
-	vpxor 2*16+8(%r12), RL2, RL2;
-	vpxor 3*16+8(%r12), RR3, RR3;
-	vpxor 4*16+8(%r12), RL3, RL3;
-	vpxor 5*16+8(%r12), RR4, RR4;
-	vpxor 6*16+8(%r12), RL4, RL4;
+	vpxor 0*16+8(%r14), RL1, RL1;
+	vpxor 1*16+8(%r14), RR2, RR2;
+	vpxor 2*16+8(%r14), RL2, RL2;
+	vpxor 3*16+8(%r14), RR3, RR3;
+	vpxor 4*16+8(%r14), RL3, RL3;
+	vpxor 5*16+8(%r14), RR4, RR4;
+	vpxor 6*16+8(%r14), RL4, RL4;
 
 	vmovdqu RR1, (0*16)(%r11);
 	vmovdqu RL1, (1*16)(%r11);
@@ -483,10 +484,10 @@ ENTRY(cast5_cbc_dec_16way)
 	vmovdqu RR4, (6*16)(%r11);
 	vmovdqu RL4, (7*16)(%r11);
 
-	popq %r12;
+	popq %r14;
 
 	FRAME_END
-	ret;
+	pax_ret cast5_cbc_dec_16way;
 ENDPROC(cast5_cbc_dec_16way)
 
 ENTRY(cast5_ctr_16way)
@@ -498,10 +499,10 @@ ENTRY(cast5_ctr_16way)
 	 */
 	FRAME_BEGIN
 
-	pushq %r12;
+	pushq %r14;
 
 	movq %rsi, %r11;
-	movq %rdx, %r12;
+	movq %rdx, %r14;
 
 	vpcmpeqd RTMP, RTMP, RTMP;
 	vpsrldq $8, RTMP, RTMP; /* low: -1, high: 0 */
@@ -538,17 +539,17 @@ ENTRY(cast5_ctr_16way)
 	vpshufb R1ST, RX, RX; /* be: IV16, IV16 */
 	vmovq RX, (%rcx);
 
-	call __cast5_enc_blk16;
+	pax_direct_call __cast5_enc_blk16;
 
 	/* dst = src ^ iv */
-	vpxor (0*16)(%r12), RR1, RR1;
-	vpxor (1*16)(%r12), RL1, RL1;
-	vpxor (2*16)(%r12), RR2, RR2;
-	vpxor (3*16)(%r12), RL2, RL2;
-	vpxor (4*16)(%r12), RR3, RR3;
-	vpxor (5*16)(%r12), RL3, RL3;
-	vpxor (6*16)(%r12), RR4, RR4;
-	vpxor (7*16)(%r12), RL4, RL4;
+	vpxor (0*16)(%r14), RR1, RR1;
+	vpxor (1*16)(%r14), RL1, RL1;
+	vpxor (2*16)(%r14), RR2, RR2;
+	vpxor (3*16)(%r14), RL2, RL2;
+	vpxor (4*16)(%r14), RR3, RR3;
+	vpxor (5*16)(%r14), RL3, RL3;
+	vpxor (6*16)(%r14), RR4, RR4;
+	vpxor (7*16)(%r14), RL4, RL4;
 	vmovdqu RR1, (0*16)(%r11);
 	vmovdqu RL1, (1*16)(%r11);
 	vmovdqu RR2, (2*16)(%r11);
@@ -558,8 +559,8 @@ ENTRY(cast5_ctr_16way)
 	vmovdqu RR4, (6*16)(%r11);
 	vmovdqu RL4, (7*16)(%r11);
 
-	popq %r12;
+	popq %r14;
 
 	FRAME_END
-	ret;
+	pax_ret cast5_ctr_16way;
 ENDPROC(cast5_ctr_16way)
